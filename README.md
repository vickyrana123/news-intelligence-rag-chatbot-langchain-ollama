# ğŸ“° News Intelligence Chatbot â€” RAG Pipeline

A production-style **Retrieval-Augmented Generation (RAG)** chatbot that fetches real-time news articles via NewsAPI, stores them in a vector database, and answers questions using a local LLM (Ollama). Built with LangChain, ChromaDB, FastAPI, and Streamlit.

---

## ğŸ—ï¸ Architecture

```
NewsAPI â†’ ingest.py â†’ ChromaDB (vector store)
                              â†“
User Question â†’ rag.py â†’ similarity search â†’ Ollama LLM â†’ Answer
                              â†“
                         app.py (FastAPI) â† streamlit_app.py (UI)
```

---

## ğŸ“ Project Structure

```
news_rag_chatbot/
â”œâ”€â”€ ingest.py           # Fetch news, chunk, embed, store in ChromaDB
â”œâ”€â”€ rag.py              # Retrieval + LLM answer generation
â”œâ”€â”€ app.py              # FastAPI REST API
â”œâ”€â”€ streamlit_app.py    # Streamlit chat UI
â”œâ”€â”€ .env                # API keys (never commit this)
â”œâ”€â”€ .env.example        # Template for .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup & Installation

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/news-rag-chatbot.git
cd news-rag-chatbot
```

### 2. Create and activate virtual environment
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Mac/Linux
source venv/bin/activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Set up environment variables
```bash
cp .env.example .env
```
Edit `.env` and add your NewsAPI key:
```
NEWS_API_KEY=your_key_here
```
Get a free API key at [https://newsapi.org](https://newsapi.org)

### 5. Install and start Ollama
Download from [https://ollama.com/download](https://ollama.com/download), then:
```bash
ollama pull llama3.2:1b
```
Ollama starts automatically on Windows after installation.

---

## ğŸš€ Running the Project

### Step 1 â€” Ingest news articles
```bash
python ingest.py
```
This fetches the latest Tesla news, chunks and embeds the articles, and saves them to the `db/` folder.

### Step 2 â€” Start the FastAPI backend
```bash
python app.py
```
API will be available at `http://localhost:8000`  
Interactive docs at `http://localhost:8000/docs`

### Step 3 â€” Launch the Streamlit UI
Open a new terminal, activate the venv, then:
```bash
streamlit run streamlit_app.py
```
Open `http://localhost:8501` in your browser.

---

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET    | `/`      | Health check |
| POST   | `/ask`   | Ask a question |
| POST   | `/clear` | Reset chat history and cache |

**Example request:**
```bash
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "What is Tesla latest news?"}'
```

**Example response:**
```json
{
  "question": "What is Tesla latest news?",
  "answer": "Tesla recently announced... \n\nğŸ“° Sources: Reuters, Electrek"
}
```

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| News data | NewsAPI |
| Text splitting | LangChain RecursiveCharacterTextSplitter |
| Embeddings | HuggingFace `all-MiniLM-L6-v2` |
| Vector store | ChromaDB |
| LLM | Ollama (`llama3.2:1b`) |
| Backend API | FastAPI + Uvicorn |
| Frontend | Streamlit |

---

## ğŸ“¦ Requirements

```
requests
python-dotenv
langchain-text-splitters
langchain-huggingface
langchain-community
langchain-chroma
langchain-core
langchain-ollama
chromadb
sentence-transformers
fastapi
uvicorn
streamlit
```

---

## ğŸ”§ Configuration

You can change the news topic by passing a different query when running ingest:

```python
# In ingest.py, change the default query
ingest_documents(query="apple")  # or "AI", "bitcoin", etc.
```

---

## ğŸ“Œ Notes

- The `db/` folder is created automatically after running `ingest.py`
- Re-run `ingest.py` any time to refresh the knowledge base with the latest articles
- The LLM only answers from retrieved context â€” it will say "I don't have enough information" if the answer isn't in the news articles
- Responses are cached per session to avoid redundant LLM calls

---

## ğŸ‘¤ Author

Built by **VICKY RANA**  
[LinkedIn](https://www.linkedin.com/in/vicky-rana-125084225/) Â· [GitHub](https://github.com/vickyrana123)
